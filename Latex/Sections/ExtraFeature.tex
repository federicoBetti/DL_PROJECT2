We have added some extra features that we think could be useful for an easier and more complete use of the framework, even in such a prototype version.
\begin{itemize}
	\item \textbf{Softmax}: We decided to implement the Softmax activation function because it is the one that best fits a classification problem since it relates all the output layers, creating a probability vector. However, in the final solution it was not used as it achieves the best performance with the CrossEntropy Loss; with a MSE Loss a Sigmoid output function still has better results.
	\item \textbf{Compute History}: This framework feature allows you to keep track of epoch-wise loss and accuracy for both training dataset and validation dataset. It can be very useful to see the network trend, to study overfitting and to compare different models. Since at any time it has to calculate loss and accuracy for both dataset, it slightly slows down the training process so we have inserted the option to disable it.
	\item \textbf{Real-time Graphic Visualization}: We have provided the possibility of having an epoch-wise graphic display of the network trend, showing which are the correct and wrong predicted samples. This tool can be very useful because it allows you to see the evolution of the hyperplanes that delimit the classes in each epoch and consequently it gives you a clear visual example of how the process evolves.
\end{itemize}

Although this is only a starting project we discussed what future implementations could be easily developed to add useful functionality to the framework:
\begin{itemize}
	\item \textbf{CrossEntropy Loss}: This is another type of loss function that is suitable for classification problems and can be used easily and with excellent results with the softmax activation function. We believe that this is the first thing to develop in the future.
	\item \textbf{Dropout}: Adding dropout layer to the framework should not be difficult to implement and could be very useful to tackle more complicated problems, where the risk of overfitting increases significantly with the noise in the data.
	\item \textbf{Other optimizers}: Several methods of optimization alternative to SGD have been invented and often have better performances (Adam, Adadelta...). Therefore, they could be objects of future expansions of the framework.
\end{itemize}
